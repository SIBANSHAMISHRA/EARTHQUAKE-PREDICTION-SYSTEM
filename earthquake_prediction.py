# -*- coding: utf-8 -*-
"""Earthquake_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12cc-2q8YXCLbUqb2bbQIoc-_SntUa8Mo
"""

!pip install fastapi nest-asyncio pyngrok uvicorn pydantic firebase-admin scikit-learn

import nest_asyncio
from pyngrok import ngrok
import threading
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
import requests
import time
import pickle
import firebase_admin
from firebase_admin import credentials, firestore
import os

from sklearn.datasets import make_classification, make_regression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import pickle


X_class, y_class = make_classification(
    n_samples=100,
    n_features=5,
    n_informative=3,
    n_redundant=1,
    random_state=42
)


X_reg, y_reg = make_regression(n_samples=100, n_features=5, random_state=42)


clf = RandomForestClassifier()
clf.fit(X_class, y_class)


reg = RandomForestRegressor()
reg.fit(X_reg, y_reg)


with open('classifier.pkl', 'wb') as f:
    pickle.dump(clf, f)

with open('regressor.pkl', 'wb') as f:
    pickle.dump(reg, f)

print("Models saved successfully.")

!pip install pandas requests

import pandas as pd
import requests
from datetime import datetime, timedelta


def fetch_usgs(start, end, min_mag=2.5):
    url = "https://earthquake.usgs.gov/fdsnws/event/1/query"
    params = {
        "format": "geojson",
        "starttime": start,
        "endtime": end,
        "minmagnitude": min_mag,
        "limit": 20000
    }
    r = requests.get(url, params=params)
    data = r.json().get("features", [])
    records = []
    for feat in data:
        p = feat["properties"]
        lon, lat, depth = feat["geometry"]["coordinates"]
        records.append({
            "time": pd.to_datetime(p["time"], unit="ms"),
            "latitude": lat,
            "longitude": lon,
            "depth": depth,
            "mag": p["mag"],
            "place": p["place"]
        })
    return pd.DataFrame(records)


all_df = []
start = datetime(2024, 1, 1)
end = datetime(2025, 4, 23)
while start > datetime(2015, 1, 1) and sum(len(df) for df in all_df) < 3000:
    chunk = fetch_usgs(start.strftime("%Y-%m-%d"), end.strftime("%Y-%m-%d"))
    all_df.append(chunk)
    end = start - timedelta(days=1)
    start = end - timedelta(days=90)

df = pd.concat(all_df, ignore_index=True).drop_duplicates().reset_index(drop=True)
print("Total rows fetched:", len(df))


df = df[df.mag.notnull() & df.depth.notnull() & df.latitude.notnull() & df.longitude.notnull()]
if len(df) < 3000:
    raise RuntimeError(f"Only {len(df)} rows; adjust date ranges or min_magnitude")


df["year"]  = df["time"].dt.year
df["month"] = df["time"].dt.month
df["day"]   = df["time"].dt.day
df["hour"]  = df["time"].dt.hour

def classify_severity(m):
    if m < 4.0:   return "mild"
    if m < 6.0:   return "moderate"
    return "severe"

df["severity"] = df["mag"].apply(classify_severity)


out_cols = [
    "time", "year", "month", "day", "hour",
    "latitude", "longitude", "depth",
    "mag", "severity", "place"
]
df[out_cols].to_csv("earthquake_3000_plus.csv", index=False)
print("Saved earthquake_3000_plus.csv with columns:", out_cols)

from google.colab import files


files.download('earthquake_3000_plus.csv')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd


df = pd.read_csv('earthquake_3000_plus.csv')


df.head()

import os, pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# 1. Create models folder
os.makedirs('models', exist_ok=True)

# 2. Features and targets
X = df[['latitude', 'longitude', 'depth']].values
y_clf = df['severity'].values
y_reg = df['mag'].values.astype(float)

# 3. Encode severity labels
le = LabelEncoder()
y_clf_enc = le.fit_transform(y_clf)

# 4. Train/test split
X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test = train_test_split(
    X, y_clf_enc, y_reg, test_size=0.2, random_state=42)

# 5. Train models
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_clf_train)

reg = RandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X_train, y_reg_train)

# 6. Save classifier + label encoder together
with open('models/classifier.pkl', 'wb') as f:
    pickle.dump((clf, le), f)

# 7. Save regressor
with open('models/regressor.pkl', 'wb') as f:
    pickle.dump(reg, f)

print(" Models saved in /models")
# Optionally check performance
print("Classification accuracy:", clf.score(X_test, y_clf_test))
from sklearn.metrics import mean_squared_error
print("Regression RMSE:", mean_squared_error(y_reg_test, reg.predict(X_test)))

from sklearn.metrics import mean_squared_error
import numpy as np

mse = mean_squared_error(y_reg_test, reg.predict(X_test))
rmse = np.sqrt(mse)
print("Regression RMSE:", rmse)

import os, pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor


os.makedirs('models', exist_ok=True)


X = df[['latitude', 'longitude', 'depth']].values
y_clf = df['severity'].values
y_reg = df['mag'].values.astype(float)


le = LabelEncoder()
y_clf_enc = le.fit_transform(y_clf)


X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test = train_test_split(
    X, y_clf_enc, y_reg, test_size=0.2, random_state=42)


clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_clf_train)

reg = RandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X_train, y_reg_train)


with open('models/classifier.pkl', 'wb') as f:
    pickle.dump((clf, le), f)


with open('models/regressor.pkl', 'wb') as f:
    pickle.dump(reg, f)

print(" Models saved in /models")

print("Classification accuracy:", clf.score(X_test, y_clf_test))
from sklearn.metrics import mean_squared_error
print("Regression RMSE:", mean_squared_error(y_reg_test, reg.predict(X_test)))

from sklearn.metrics import mean_squared_error
import numpy as np

mse = mean_squared_error(y_reg_test, reg.predict(X_test))
rmse = np.sqrt(mse)
print("Regression RMSE:", rmse)

import firebase_admin
from firebase_admin import credentials, firestore


cred = credentials.Certificate("earth.json")


if not firebase_admin._apps:
    firebase_admin.initialize_app(cred)


db = firestore.client()

example_data = {
    'latitude': 34.05,
    'longitude': -118.24,
    'depth': 10,
    'severity_prediction': 'moderate',
    'magnitude_prediction': 5.7
}


db.collection('earthquake_predictions').add(example_data)

import firebase_admin
from firebase_admin import credentials, firestore


if not firebase_admin._apps:
    cred = credentials.Certificate("earth.json")
    firebase_admin.initialize_app(cred)


db = firestore.client()

!pip install numpy pandas scikit-learn matplotlib seaborn tensorflow keras opencv-python firebase-admin

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# numpy
# pandas
# scikit-learn
# matplotlib
# seaborn
# tensorflow
# keras
# opencv-python
# firebase-admin
#

!pip install -r requirements.txt

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("earthquake_3000_plus (2).csv")
df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


df = pd.read_csv("earthquake_3000_plus (2).csv")


le = LabelEncoder()
df['place_encoded'] = le.fit_transform(df['place'])


features = ['latitude', 'longitude', 'depth', 'year', 'month', 'day', 'hour', 'place_encoded']


df['severity'] = le.fit_transform(df['severity'])
X_clf = df[features]
y_clf = df['severity']


X_reg = df[features]
y_reg = df['mag']


X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)
X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

from sklearn.model_selection import train_test_split


X = df.drop(['mag'], axis=1)
y_reg = df['mag']
y_clf = (df['mag'] >= 5).astype(int)

X_train, X_test, y_reg_train, y_reg_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)
_, _, y_clf_train, y_clf_test = train_test_split(X, y_clf, test_size=0.2, random_state=42)

!pip install pandas requests


import pandas as pd
import requests
from datetime import datetime, timedelta


orig = pd.read_csv("earthquake_3000_plus (2).csv", parse_dates=["time"])
print("Original rows:", len(orig))
print(orig.mag.describe())


def fetch_usgs_mag6(start, end):
    url = "https://earthquake.usgs.gov/fdsnws/event/1/query"
    params = {
        "format": "geojson",
        "starttime": start,
        "endtime": end,
        "minmagnitude": 6.0,
        "limit": 20000
    }
    r = requests.get(url, params=params)
    data = r.json().get("features", [])
    recs = []
    for feat in data:
        p = feat["properties"]
        lon, lat, depth = feat["geometry"]["coordinates"]
        recs.append({
            "time": pd.to_datetime(p["time"], unit="ms"),
            "latitude": lat,
            "longitude": lon,
            "depth": depth,
            "mag": p["mag"],
            "place": p["place"]
        })
    return pd.DataFrame(recs)


frames = [orig]
end = datetime.utcnow()
start = end - timedelta(days=5*365)
while start < end:
    chunk_end = start + timedelta(days=180)
    df6 = fetch_usgs_mag6(start.strftime("%Y-%m-%d"), chunk_end.strftime("%Y-%m-%d"))
    frames.append(df6)
    start = chunk_end + timedelta(days=1)


full = pd.concat(frames, ignore_index=True)
full.drop_duplicates(subset=["time","latitude","longitude"], inplace=True)
full.dropna(subset=["mag","depth"], inplace=True)
print("Merged rows:", len(full))
print("Magnitudes now range:", full.mag.min(), "to", full.mag.max())


full["year"]  = full.time.dt.year
full["month"] = full.time.dt.month
full["day"]   = full.time.dt.day
full["hour"]  = full.time.dt.hour

def label_severity(mag, depth):
    if mag >= 6.0 and depth <= 100:
        return "severe"
    elif 4.5 <= mag < 6.0 and depth <= 300:
        return "moderate"
    else:
        return "mild"


full["severity"] = full.apply(lambda row: label_severity(row["mag"], row["depth"]), axis=1)




out_cols = ["time","year","month","day","hour","latitude","longitude","depth","mag","severity","place"]
full[out_cols].to_csv("earthquake_with_severity.csv", index=False)
print("Saved final dataset with all three classes:", full.severity.value_counts())

print(full["severity"].value_counts())

import pandas as pd


df = pd.read_csv("earthquake_with_severity.csv", parse_dates=["time"])
print("Rows:", len(df))
df.head()

df['hour']   = df['time'].dt.hour
df['minute'] = df['time'].dt.minute
df['second'] = df['time'].dt.second


X = df.drop(columns=['time','place','mag','severity'])


y_reg = df['mag']


severity_map = {'mild':0, 'moderate':1, 'severe':2}
y_clf = df['severity'].map(severity_map)


print("Feature shape:", X.shape)
print("Regression target shape:", y_reg.shape)
print("Classification distribution:\n", df['severity'].value_counts())

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt


features = ['year', 'month', 'day', 'hour', 'latitude', 'longitude', 'depth']


df['high_impact'] = (df['mag'] >= 5).astype(int)

X = df[features]
y_clf = df['high_impact']


X_train, X_test, y_clf_train, y_clf_test = train_test_split(X, y_clf, test_size=0.2, random_state=42)


clf_model = RandomForestClassifier()
clf_model.fit(X_train, y_clf_train)


clf_preds = clf_model.predict(X_test)


print(" Classification Report:\n", classification_report(y_clf_test, clf_preds))


cm = confusion_matrix(y_clf_test, clf_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Low", "High"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error,
    accuracy_score, classification_report,
    confusion_matrix, ConfusionMatrixDisplay
)


features = ['year','month','day','hour','latitude','longitude','depth']
X = df[features]
y_reg = df['mag']
y_clf = (df['mag'] >= 5).astype(int)


X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X, y_reg, test_size=0.2, random_state=42
)
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X, y_clf, test_size=0.2, random_state=42
)


reg = RandomForestRegressor(random_state=42).fit(X_train_reg, y_train_reg)
clf = RandomForestClassifier(random_state=42).fit(X_train_clf, y_train_clf)


y_pred_reg = reg.predict(X_test_reg)
mae  = mean_absolute_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
print(f"ðŸ”¹ Regression MAE:  {mae:.3f}")
print(f"ðŸ”¹ Regression RMSE: {rmse:.3f}")


plt.figure(figsize=(6,6))
plt.scatter(y_test_reg, y_pred_reg, alpha=0.5)
lims = [min(y_test_reg.min(), y_pred_reg.min()), max(y_test_reg.max(), y_pred_reg.max())]
plt.plot(lims, lims, 'r--')
plt.xlabel("Actual Magnitude"); plt.ylabel("Predicted Magnitude")
plt.title("Regression: Predicted vs Actual")
plt.show()


residuals = y_test_reg - y_pred_reg
plt.figure(figsize=(6,4))
sns.histplot(residuals, bins=30, kde=True)
plt.title("Regression Residuals Distribution")
plt.xlabel("Residual")
plt.show()


importances = reg.feature_importances_
feat_names = X_train_reg.columns
feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)
plt.figure(figsize=(8,5))
sns.barplot(x=feat_imp.values, y=feat_imp.index)
plt.title("Feature Importance for Magnitude Regression")
plt.xlabel("Importance"); plt.ylabel("Feature")
plt.show()



y_pred_clf = clf.predict(X_test_clf)
acc = accuracy_score(y_test_clf, y_pred_clf)
print(f"ðŸ”¹ Classification Accuracy: {acc:.3f}\n")
print("Classification Report:\n", classification_report(y_test_clf, y_pred_clf, target_names=['Low','High']))


cm = confusion_matrix(y_test_clf, y_pred_clf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Low','High'])
disp.plot(cmap="Blues")
plt.title("Classification Confusion Matrix")
plt.show()


comp = pd.DataFrame({'Actual':y_test_clf, 'Predicted':y_pred_clf})
plt.figure(figsize=(6,4))
sns.countplot(x='Actual', data=comp, order=[0,1], label='Actual')
sns.countplot(x='Predicted', data=comp, order=[0,1], label='Predicted', alpha=0.7)
plt.xticks([0,1], ['Low','High'])
plt.title("Actual vs Predicted Class Counts")
plt.legend(); plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_clf_test, clf_preds))

df['severity'].value_counts().plot(kind='bar', title='Severity Distribution')

importances = clf_model.feature_importances_
pd.Series(importances, index=X.columns).sort_values().plot(kind='barh')

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split


X = full[["latitude", "longitude", "depth"]]
y = full["mag"]


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


reg_model = LinearRegression()
reg_model.fit(X_train, y_train)


import joblib
joblib.dump(reg_model, "magnitude_regressor.pkl")

import joblib
joblib.dump(clf_model, "earthquake_classifier.pkl")
joblib.dump(reg_model, "magnitude_regressor.pkl")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import pandas as pd


X = df[features]
y_reg = df['mag']
y_clf = (df['mag'] >= 5).astype(int)

X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(
    X, y_reg, y_clf, test_size=0.2, random_state=42
)


reg = RandomForestRegressor(random_state=42).fit(X_train, y_reg_train)
clf = RandomForestClassifier(random_state=42).fit(X_train, y_clf_train)


reg_preds = reg.predict(X_test)
clf_preds = clf.predict(X_test)


preds_df = pd.DataFrame({
    "Actual_Magnitude":    y_reg_test.values,
    "Predicted_Magnitude": reg_preds,
    "Actual_Severity":     y_clf_test.values,
    "Predicted_Severity":  clf_preds
})


print(preds_df.shape)
preds_df.head()

import joblib
joblib.dump(reg, 'regressor_model.pkl')
joblib.dump(clf, 'classifier_model.pkl')

import joblib
joblib.dump(reg, 'magnitude_regressor.pkl')
joblib.dump(clf, 'severity_classifier.pkl')

preds_df.to_csv('earthquake_model_predictions.csv', index=False)

import pandas as pd
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib


df = pd.read_csv("earthquake_with_severity.csv")


features = ["latitude", "longitude", "depth"]
X = df[features]
y_reg = df["mag"]
y_clf_raw = df["severity"]


le = LabelEncoder()
y_clf = le.fit_transform(y_clf_raw)


reg = RandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X, y_reg)


clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X, y_clf)


joblib.dump(reg, "regressor.pkl")
joblib.dump((clf, le), "classifier.pkl")

print(" Models saved: regressor.pkl, classifier.pkl")

from google.colab import files
files.download("regressor.pkl")
files.download("classifier.pkl")

print(df['severity'].value_counts())

import os
print(os.listdir())

from textblob import TextBlob

def analyze_sentiment(text):
    blob = TextBlob(text)
    sentiment = blob.sentiment.polarity
    if sentiment < 0:
        return "Negative sentiment detected, earthquake might be imminent."
    elif sentiment > 0:
        return "Positive sentiment detected, no immediate concern."
    else:
        return "Neutral sentiment."


text = "A massive earthquake is expected soon in this region."
print(analyze_sentiment(text))

from transformers import pipeline

nlp = pipeline('sentiment-analysis')

text = "Urgent: Earthquake warning in the coastal region!"
result = nlp(text)
print(result)

!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace earthquake_prediction.ipynb